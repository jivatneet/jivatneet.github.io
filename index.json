[{"authors":null,"categories":null,"content":"Hi! I am a Predoctoral Research Fellow at Microsoft Research (MSR), where I am working with Dr. Amit Sharma and Dr. Emre Kiciman. My research focuses on causal representation learning for Out-of-Distribution generalization. I am broadly interested in developing machine learning systems that are robust to data distribution shifts in real-world deployments.\nI graduated from BITS Pilani, Pilani with a Bachelor\u0026rsquo;s in Computer Science in 2021. I was fortunate to pursue my undergraduate thesis at the MultiComp Lab in the Language Technologies Institute of Carnegie Mellon University, supervised by Prof. Louis-Philippe Morency and mentored by Paul Pu Liang and Yiding Jiang. My thesis was focused on multimodal reinforcement learning, specifically exploring how language grounding can assist in capturing affordance of objects and accelerate learning of autonomous agents.\nPrior to MSR, I was a research intern at Adobe\u0026rsquo;s Media and Data Science Research (MDSR) lab, working on knowledge enhancement of language models to make robust factual and commonsense reasoning-aware predictions. I have also had the good fortune of working with Prof. Dr. Chris Biemann at Language Technology Lab, Universität Hamburg on semantic parsing for knowledge graph question answering. In the past, I have interned as a software engineer at Microsoft and worked on computer vision problems at MapmyIndia.\nWay back, I was obsessed with being an astronaut (maybe someday?) and my fascination with space led me to joining Team Anant, a group of passionate undergraduate students building BITS Pilani\u0026rsquo;s first nanosatellite, where I had a great time designing control algorithms and brainstorming with some amazing people. I love running, playing basketball, and am trying to find time to revisit my passion to write.\n","date":1674321202,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674321202,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://jivatneet.github.io/author/jivat-neet-kaur/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jivat-neet-kaur/","section":"authors","summary":"Hi! I am a Predoctoral Research Fellow at Microsoft Research (MSR), where I am working with Dr. Amit Sharma and Dr. Emre Kiciman. My research focuses on causal representation learning for Out-of-Distribution generalization.","tags":null,"title":"Jivat Neet Kaur","type":"authors"},{"authors":null,"categories":null,"content":"Hi! I am a Research Fellow at Microsoft Research, where I am working with Dr. Amit Sharma and Dr. Emre Kiciman. My research focuses on causal representation learning and I am broadly interested in developing robust and generalizable systems possessing grounded language understanding.\nPrior to this, I was a research intern at Adobe\u0026rsquo;s Media and Data Science Research (MDSR) lab, where I worked on knowledge enhancement of language models to make robust factual predictions. I also contributed to a project on task-agnostic generalizable commonsense reasoning.\nI graduated from BITS Pilani, Pilani with a Bachelor\u0026rsquo;s in Computer Science in 2021. I was fortunate to pursue my undergraduate thesis at the MultiComp Lab in the Language Technologies Institute of Carnegie Mellon University, supervised by Prof. Louis-Philippe Morency and mentored by Paul Pu Liang and Yiding Jiang. My thesis was focused on multimodal reinforcement learning, specifically exploring how language grounding can assist in capturing affordance of objects and accelerate learning of autonomous agents.\nIn the past, I collaborated with the Language Technology Lab, Universität Hamburg, where I felt grateful to be working under the supervision of Prof. Dr. Chris Biemann on query building and question answering over knowledge graphs. Previously, I have interned as a software engineer at Microsoft, Bangalore and worked on computer vision problems at MapmyIndia, Delhi.\nPersonal: My childhood fascination with space (still excites me) led me to joining Team Anant, a group of passionate undergraduate students building BITS Pilani\u0026rsquo;s first nanosatellite, where I was fortunate to work with some amazing people. I love running, playing basketball and am trying to find time to revisit my passion to write.\n","date":1657559602,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1657559602,"objectID":"372986362fa59a1284a53653b609e312","permalink":"https://jivatneet.github.io/author/jivat-neet-kaur/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jivat-neet-kaur/","section":"authors","summary":"Hi! I am a Research Fellow at Microsoft Research, where I am working with Dr. Amit Sharma and Dr. Emre Kiciman. My research focuses on causal representation learning and I am broadly interested in developing robust and generalizable systems possessing grounded language understanding.","tags":null,"title":"Jivat Neet Kaur*","type":"authors"},{"authors":["Jivat Neet Kaur","Emre Kiciman","Amit Sharma"],"categories":[],"content":"","date":1674321202,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674321202,"objectID":"bf80571c7fdac80740f6cee8533a0831","permalink":"https://jivatneet.github.io/publication/cacm/","publishdate":"2022-01-21T22:43:22+05:30","relpermalink":"/publication/cacm/","section":"publication","summary":"Recent empirical studies on domain generalization (DG) have shown that DG algorithms that perform well on some distribution shifts fail on others, and no state-of-the-art DG algorithm performs consistently well on all shifts. Moreover, real-world data often has multiple distribution shifts over different attributes; hence we introduce multi-attribute distribution shift datasets and find that the accuracy of existing DG algorithms falls even further. To explain these results, we provide a formal characterization of generalization under multi-attribute shifts using a canonical causal graph. Based on the relationship between spurious attributes and the classification label, we obtain realizations of the canonical causal graph that characterize common distribution shifts and show that each shift entails different independence constraints over observed variables. As a result, we prove that any algorithm based on a single, fixed constraint cannot work well across all shifts, providing theoretical evidence for mixed empirical results on DG algorithms. Based on this insight, we develop *Causally Adaptive Constraint Minimization (CACM)*, an algorithm that uses knowledge about the data-generating process to adaptively identify and apply the correct independence constraints for regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds datasets, covering binary and multi-valued attributes and labels, show that adaptive dataset-dependent constraints lead to the highest accuracy on unseen domains whereas incorrect constraints fail to do so. Our results demonstrate the importance of modeling the causal relationships inherent in the data-generating process.","tags":[],"title":"Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization","type":"publication"},{"authors":["Jivat Neet Kaur","Sumit Bhatia","Milan Aggarwal","Rachit Bansal","Balaji Krishnamurthy"],"categories":[],"content":"","date":1657559602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657559602,"objectID":"5a0eae2c8a13a57556c0a47f4841697f","permalink":"https://jivatneet.github.io/publication/knowledgebert/","publishdate":"2022-07-11T22:43:22+05:30","relpermalink":"/publication/knowledgebert/","section":"publication","summary":"Large transformer-based pre-trained language models have achieved impressive performance on a variety of knowledge-intensive tasks and can capture factual knowledge in their parameters. We argue that storing large amounts of knowledge in the model parameters is sub-optimal given the ever-growing amounts of knowledge and resource requirements. We posit that a more efficient alternative is to provide explicit access to contextually relevant structured knowledge to the model and train it to use that knowledge. We present LM-CORE -- a general framework to achieve this -- that allows \textit{decoupling} of the language model training from the external knowledge source and allows the latter to be updated without affecting the already trained model. Experimental results show that LM-CORE, having access to external knowledge, achieves significant and robust outperformance over state-of-the-art knowledge-enhanced language models on knowledge probing tasks; can effectively handle knowledge updates; and performs well on two downstream tasks. We also present a thorough error analysis highlighting the successes and failures of LM-CORE.","tags":[],"title":"LM-CORE: Language Models with Contextually Relevant External Knowledge","type":"publication"},{"authors":["Debayan Banerjee","Jivat Neet Kaur*","Pranav Ajit Nair*","Ricardo Usbeck","Chris Biemann"],"categories":[],"content":"","date":1657559602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657559602,"objectID":"520511c9af00d653f13256b784bb001b","permalink":"https://jivatneet.github.io/publication/sigir_sparql/","publishdate":"2022-07-11T22:43:22+05:30","relpermalink":"/publication/sigir_sparql/","section":"publication","summary":"In this work, we focus on the task of generating SPARQL queries from natural language questions, which can then be executed on Knowledge Graphs (KGs). We assume that gold entity and relations have been provided, and the remaining task is to arrange them in the right order along with SPARQL vocabulary, and input tokens to produce the correct SPARQL query. Pre-trained Language Models (PLMs) have not been explored in depth on this task so far, so we experiment with BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings, looking for new baselines in the PLM era for this task, on DBpedia and Wikidata KGs. We show that T5 requires special input tokenisation, but produces state of the art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms task-specific models from previous works. Moreover, the methods enable semantic parsing for questions where a part of the input needs to be copied to the output query, thus enabling a new paradigm in KG semantic parsing.","tags":[],"title":"Modern Baselines for SPARQL Semantic Parsing","type":"publication"},{"authors":["Rachit Bansal","Milan Aggarwal","Sumit Bhatia","Jivat Neet Kaur","Balaji Krishnamurthy"],"categories":[],"content":"","date":1657473202,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657473202,"objectID":"745a913e357b1e82897685898fb79917","permalink":"https://jivatneet.github.io/publication/coseco/","publishdate":"2021-07-10T22:43:22+05:30","relpermalink":"/publication/coseco/","section":"publication","summary":"Pre-trained Language Models (PTLMs) have been shown to perform well on natural language tasks. Many prior works have leveraged structured commonsense present in the form of entities linked through labeled relations in Knowledge Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static module which limits coverage since KGs contain finite knowledge. Generative methods train PTLMs on KG triples to improve the scale at which knowledge can be obtained. However, training on symbolic KG entities limits their applicability in tasks involving natural language text where they ignore overall context. To mitigate this, we propose a CommonSense Contextualizer (CoSe-Co) conditioned on sentences as input to make it generically usable in tasks for generating knowledge relevant to the overall context of input text. To train CoSe-Co, we propose a novel dataset comprising of sentence and commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and contain novel entities not present in the underlying KG. We augment generated knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets. We also demonstrate its applicability in improving performance of a baseline model for paraphrase generation task.","tags":[],"title":"CoSe-Co: Text Conditioned Generative CommonSense Contextualizer","type":"publication"},{"authors":[],"categories":[],"content":"","date":1639074923,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639074923,"objectID":"99478c9b33369327432b7c60b2f4f17c","permalink":"https://jivatneet.github.io/project/curiosity/","publishdate":"2021-12-10T00:05:23+05:30","relpermalink":"/project/curiosity/","section":"project","summary":"Worked on improving agent exploration in sparse reward environments by formulating structured intrinsic rewards. Devised a novel form of curiosity leveraging *grounded question answering* to encourage the agent to ask questions about the environment and be curious when the answers to these questions change.","tags":[],"title":"Language Models for Curiosity-driven Exploration","type":"project"},{"authors":["Jivat Neet Kaur","Yiding Jiang","Paul Pu Liang"],"categories":[],"content":"","date":1621617202,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621617202,"objectID":"f4834bee4a188735ad03c9844a512ca6","permalink":"https://jivatneet.github.io/publication/iclr-eml/","publishdate":"2021-05-26T22:43:22+05:30","relpermalink":"/publication/iclr-eml/","section":"publication","summary":"In many real-world scenarios where extrinsic rewards to the agent are extremely sparse, curiosity has emerged as a useful concept providing intrinsic rewards that enable the agent to explore its environment and acquire knowledge to achieve its goals. Despite their strong performance on many sparse-reward tasks, existing curiosity approaches rely on a holistic view of state transitions and do not allow for a structured understanding of specific aspects of the environment. In this paper, we formulate curiosity based on grounded question answering by encouraging the agent to ask questions about the environment and be curious when the answers to their questions change. We show that language questions encourage the agent to uncover specific knowledge about their environment such as the physical properties of objects as well as their spatial relationships with other objects, which serve as valuable curiosity rewards to solve sparse-reward tasks more efficiently.","tags":[],"title":"Ask \u0026 Explore: Grounded Question Answering for Curiosity-driven exploration","type":"publication"},{"authors":[],"categories":[],"content":"","date":1608404607,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608404607,"objectID":"c6936d3873c5d574db9dc5e0a11e412f","permalink":"https://jivatneet.github.io/project/covinfo/","publishdate":"2020-12-20T00:33:27+05:30","relpermalink":"/project/covinfo/","section":"project","summary":"Developed a web application for real-time hospital resource monitoring (beds, ICUs, ventilators). Integrated a mask detection model to provide real-time information regarding the percentage of people wearing masks at any location using live video feed.","tags":[],"title":"COVINFO Application","type":"project"},{"authors":[],"categories":[],"content":"Built a multi-scale deep CNN architecture using Keras to learn Deep Motor Features for brain computer interface with imagined motor tasks in BCI - EEG motor activity dataset.\n","date":1607611783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607611783,"objectID":"7eefeeea8d92fd457e5ccc545c64dc6c","permalink":"https://jivatneet.github.io/extra_content/bci/","publishdate":"2020-12-10T20:19:43+05:30","relpermalink":"/extra_content/bci/","section":"extra_content","summary":"Built a multi-scale deep CNN architecture using Keras to learn Deep Motor Features for brain computer interface with imagined motor tasks in BCI - EEG motor activity dataset.","tags":[],"title":"Deep Learning EEG Response Representation for BCI","type":"extra_content"},{"authors":[],"categories":[],"content":"","date":1607543802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607543802,"objectID":"ce22aef0df50d01d0bcf84d4266f5323","permalink":"https://jivatneet.github.io/extra_content/socketprog/","publishdate":"2020-12-10T01:26:42+05:30","relpermalink":"/extra_content/socketprog/","section":"extra_content","summary":"Implementation of File transfer using multi-channel stop-and-wait (TCP) and Selective Repeat (UDP) protocols.","tags":[],"title":"Socket Programming","type":"extra_content"},{"authors":[],"categories":[],"content":"","date":1607540081,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607540081,"objectID":"278ed1ae97bcf2488a8841c384cc4afe","permalink":"https://jivatneet.github.io/extra_content/ner/","publishdate":"2020-12-10T00:24:41+05:30","relpermalink":"/extra_content/ner/","section":"extra_content","summary":"Trained word embeddings using Word2Vec's Skip-Gram model and created a window-based classification Network in Python for learning Named Entity classes.","tags":[],"title":"Named entity recognition (NER)","type":"extra_content"},{"authors":[],"categories":[],"content":"","date":1607538923,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607538923,"objectID":"b0fad77f507e2f8538af926b6ba0f335","permalink":"https://jivatneet.github.io/project/compiler/","publishdate":"2020-12-10T00:05:23+05:30","relpermalink":"/project/compiler/","section":"project","summary":"Developed a fully functional compiler from scratch (in C) capable of lexical analysis, syntax tree creation, semantic analysis, static and dynamic type checking and generating executable assembly code. The artificial language supported constructs like dynamic memory allocation, loops, if-else ladders, switch statements, nested scopes and function calls.","tags":[],"title":"Compiler Design for a Custom Language","type":"project"},{"authors":["Vishnu P Katkoori","Jivat Neet Kaur","Tushar Goyal"],"categories":[],"content":"","date":1571678002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571678002,"objectID":"ce6c6284310aae1e166ed8e168048f8c","permalink":"https://jivatneet.github.io/publication/bdot/","publishdate":"2020-12-06T22:43:22+05:30","relpermalink":"/publication/bdot/","section":"publication","summary":"As a satellite is deployed from the launch vehicle, it is subjected to high angular rates which need to be dampened in order for the satellite to perform its functions as expected. Simple and robust algorithms, such as BDot, are generally used to provide the required control torque for detumbling the satellite. This paper elucidates the design process for the detumbling algorithm to be implemented on a nanosatellite currently being developed by Team Anant, the Student Satellite Team of BITS Pilani. The process commenced with the selection of hardware to be used onboard the satellite. Magnetometers and Gyroscopes were finalized to be used as sensors. Various commercially available sensor models were then compared based on power and operating conditions. For actuation, a magnetorquer system was designed specifically to the requirements of the team. The system comprised of two magnetorquer rods and a magnetorquer coil aligned in orthogonal directions. The sensors and actuators were then accurately modeled in MATLAB for further testing. The modeling involved some interesting challenges due to the magnetic moment retained by the ferromagnetic core. These challenges, and the ways to overcome them have been also been briefly discussed in the paper. After finalizing the hardware, the team proceeded with implementing various popular control algorithms for detumbling the satellite. The algorithms were first theoretically analysed, and then modeled on MATLAB. The simulations took the space environment around the satellite into consideration for higher accuracy. The algorithms were tested for different initial conditions, using different time-steps and under different power constraints. The algorithms considered and the conclusions derived from these simulations have also been discussed elaborately in this paper. The paper concluded by presenting the finalized detumbling algorithm(s) to be used by Team Anant, and the various conditions devised to ensure efficient use of electrical power. The paper also presents viable alternatives to the finalized algorithm(s), using other hardware components. These alternatives and conditions have also been documented in the paper for a better understanding.","tags":[],"title":"Simulation and Selection of Detumbling Algorithms for a 3U CubeSat","type":"publication"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a01d6f03ca4200d35a8e800b437205a6","permalink":"https://jivatneet.github.io/project/lmcore/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/lmcore/","section":"project","summary":"Worked on knowledge enhancement of language models (LMs) by augmenting structured knowledge externally. Created a new masked pre-training corpus using Wikipedia hyperlinks to identify entity spans; trained LMs to retrieve contextually relevant knowledge via masked language modeling on this modified corpus.","tags":[],"title":"LM-CORE: Language Models with Contextually Relevant External Knowledge","type":"project"}]